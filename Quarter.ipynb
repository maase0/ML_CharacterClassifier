{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "fc6a171883505f6cf52af31efce977f648d52d3a8750cfd6622a125905379d6c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "import gc\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from etl_data_reader.etl_data_reader import ETL_data_reader\n",
    "from etl_data_reader.etl_data_names import ETL_data_names\n",
    "from etl_data_reader.etl_character_groups import ETL_character_groups\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Change matplotlib to use a Japanese font\n",
    "# to actually be able to display correctly\n",
    "matplotlib.rcParams.update(\n",
    "    {\n",
    "        'text.usetex': False,\n",
    "        'font.family': 'MS Gothic'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "source": [
    "Global variables, used throughout the program"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "num_chars = 759  #759 per part.pkl\n",
    "epochs = 150\n",
    "\n",
    "#Set the path for input data and create the reader\n",
    "path = \"./ETL_kanji\"\n",
    "reader = ETL_data_reader(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug method to see if images are loading correctly\n",
    "def show_image(img : np.array, label : str):\n",
    "    plt.figure(figsize=(2.2, 2.2))\n",
    "    plt.title(label=label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.astype(np.float64), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    sample_img, sample_lable = reader.read_dataset_file(1, ETL_data_names.ETL10, ETL_character_groups.hiragana)\n",
    "    show_image(sample_img[200], sample_lable[200])\n",
    "    # free again\n",
    "    del(sample_img)\n",
    "    del(sample_lable)"
   ]
  },
  {
   "source": [
    "## Load the data\n",
    "Grab all the data from ETL9B (ETL10) and shuffle it"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading all data set files (ETL10_x) from: ./ETL_kanji\\ETL10...\n",
      "(14271, 64, 64, 1) (14271,)\n"
     ]
    }
   ],
   "source": [
    "x, y =  reader.read_dataset_part(ETL_data_names.ETL10, ETL_character_groups.hiragana)    # load only the relevant dataset\n",
    "#x, y = reader.read_dataset_whole([ETL_character_groups.kanji], 16)\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "# because the data is ordered shuffle it\n",
    "rng_state = np.random.get_state()\n",
    "np.random.shuffle(x)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(y)"
   ]
  },
  {
   "source": [
    "One-hot-encode the labels for the CNN, save the labels for later use"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode the labels\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y)\n",
    "o_y = lb.transform(y)\n",
    "\n",
    "# free the memory from the original string labels\n",
    "del(y)\n",
    "\n",
    "# concat classes-list to string\n",
    "classes = [i for i in lb.classes_]\n",
    "s = \"\".join([i for i in lb.classes_])\n",
    "\n",
    "# save a list of all labels into a .txt file\n",
    "with open(\"./labels.txt\", \"w+\", encoding=\"utf8\") as f:\n",
    "    f.write(s)\n",
    "    \n",
    "# a string formatted as a python list can be easily evaluated \n",
    "# therefore lets save this representation too\n",
    "with open(\"./labels_python_list.txt\", \"w+\", encoding=\"utf8\") as f:\n",
    "    f.write(str(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the images in a shared array and free the loaded data\n",
    "memory_size_x = x.shape[0] * x.shape[1] * x.shape[2] * x.shape[3]\n",
    "x_shared = mp.RawArray(\"f\", memory_size_x // 2)\n",
    "x_np = np.frombuffer(x_shared, dtype=\"float16\").reshape(x.shape)\n",
    "np.copyto(x_np, x)\n",
    "del(x)\n",
    "\n",
    "# store the labels in a shared array and free the loaded data\n",
    "memory_size_y = o_y.shape[0] * o_y.shape[1]\n",
    "y_shared = mp.RawArray('b', memory_size_y)\n",
    "y_np = np.frombuffer(y_shared, dtype=\"b\").reshape(o_y.shape)\n",
    "np.copyto(y_np, o_y)\n",
    "del(o_y)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "show_image(x_np[200], lb.inverse_transform(np.array([y_np[200]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataGenerator\n",
    "\n",
    "train_generator = DataGenerator.DataGenerator(len(x_np), 2048, \n",
    "                                            80, \"training\", \n",
    "                                            x_shared, y_shared, \n",
    "                                            x_np.shape, y_np.shape,\n",
    "                                            8, True)\n",
    "test_generator = DataGenerator.DataGenerator(len(y_np), 2048, \n",
    "                                            20, \"testing\", \n",
    "                                            x_shared, y_shared, \n",
    "                                            x_np.shape, y_np.shape,\n",
    "                                            8, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = train_generator.__getitem__(10)\n",
    "test_x, test_y   = test_generator.__getitem__(10)\n",
    "\n",
    "show_image(train_x[23], lb.inverse_transform(np.array([train_y[23]])))"
   ]
  },
  {
   "source": [
    "Load data, print some for verification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(input_shape=(64, 64, 1), kernel_size=3, activation='relu', filters=32, name=\"conv2D_1_2_input\"),\n",
    "        tf.keras.layers.Conv2D(kernel_size=3, activation='relu', filters=32, name=\"conv2D_1_1\"),\n",
    "        tf.keras.layers.Conv2D(kernel_size=3, activation='relu', filters=32, name=\"conv2D_1_2\"),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=2, name=\"maxpool_1\"),\n",
    "\n",
    "        tf.keras.layers.Conv2D(kernel_size=3, activation='relu', filters=32, name=\"conv2D_2_1\"),\n",
    "        tf.keras.layers.Conv2D(kernel_size=3, activation='relu', filters=32, name=\"conv2D_2_2\"),\n",
    "\n",
    "        tf.keras.layers.Conv2D(kernel_size=3, activation='relu', filters=64, name=\"conv2D_3_1\"),\n",
    "        tf.keras.layers.Conv2D(kernel_size=3, activation='relu', filters=64, name=\"conv2D_3_2\"),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=2, name=\"maxpool_3\"),\n",
    "\n",
    "        tf.keras.layers.Conv2D(kernel_size=3, activation='relu', filters=128, name=\"conv2D_4_1\"),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=2, name=\"maxpool_4\"),\n",
    "\n",
    "        tf.keras.layers.Flatten(name=\"flatten_1\"),\n",
    "        tf.keras.layers.Dropout(0.25, name=\"dropout_1\"),\n",
    "        \n",
    "        tf.keras.layers.Dense(2048, name=\"dense_1\"),\n",
    "        tf.keras.layers.Dropout(0.1, name=\"dropout_2\"),\n",
    "        tf.keras.layers.Dense(2048, name=\"densfasave_1\"),\n",
    "        tf.keras.layers.Dropout(0.1, name=\"dropourstat_2\"),\n",
    "\n",
    "        tf.keras.layers.Dense(2048, name=\"dense_2\"),\n",
    "        tf.keras.layers.Dropout(0.25, name=\"dropout_3\"),\n",
    "        tf.keras.layers.Dense(2048, name=\"densearst_2\"),\n",
    "        tf.keras.layers.Dropout(0.25, name=\"dropouart_3\"),\n",
    "\n",
    "        tf.keras.layers.Dense(len(lb.classes_),  name=\"dense_3\"),\n",
    "\n",
    "        #set the dtype to float32 for numerical stability\n",
    "        tf.keras.layers.Softmax(dtype=\"float32\", name=\"softmax_1_output\") \n",
    "    ], name=\"model\")\n",
    "\n",
    "if debug:\n",
    "    print(model.output_shape)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path where the model should be saved\n",
    "model_dir = os.path.join(os.path.dirname(os.getcwd()), \"model\")\n",
    "print(model_dir)"
   ]
  },
  {
   "source": [
    "Train the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001,\n",
    "                                beta_1=0.9,\n",
    "                                beta_2=0.999,\n",
    "                                epsilon=1e-08,)\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    x=train_generator,\n",
    "    epochs=150,\n",
    "    initial_epoch=0,\n",
    "    validation_data=test_generator,\n",
    "    max_queue_size=400,\n",
    "    callbacks=callbacks_list,\n",
    "    workers=1)"
   ]
  },
  {
   "source": [
    "Display model information"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "\n",
    "if debug:\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    y_true = y_test\n",
    "    \n",
    "    #y_pred = le.inverse_transform(y_pred)\n",
    "    #y_true = le.inverse_transform(y_test)\n",
    "\n",
    "    labels = np.unique(le.inverse_transform(y_test))\n",
    "\n",
    "    conf_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(100, 80))\n",
    "    sns.heatmap(conf_mtx, xticklabels=labels, yticklabels=labels, \n",
    "                annot=True, fmt='g')\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('Label')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}